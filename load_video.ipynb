{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from model import Model\n",
    "\n",
    "class PermuteStack(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        d, h, w, c = sample.shape\n",
    "        assert c == 3\n",
    "        sample = sample.permute(3, 0, 1, 2)\n",
    "        return sample.reshape(c*d, h, w)\n",
    "\n",
    "class ToFloat32(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return sample.to(torch.float32) / 255.0\n",
    "\n",
    "class AdaptiveResize(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.pool(sample)\n",
    "\n",
    "class Unstack(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        cd, h, w = sample.shape\n",
    "        assert cd % 3 == 0\n",
    "        return sample.view(3, cd//3, h, w)\n",
    "\n",
    "def get_same_index(target, label):\n",
    "    label_indices = []\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == label:\n",
    "            label_indices.append(i)\n",
    "    return label_indices\n",
    "\n",
    "frames_per_clip = 8\n",
    "step_between_clips = 1\n",
    "batch_size = 16\n",
    "shuffle = True\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = Model(max_scale=4,\n",
    "              steps_per_scale=int(25e3),\n",
    "              lr=1e-3,\n",
    "              frames_per_clip=frames_per_clip)\n",
    "\n",
    "data_transform = transforms.Compose([PermuteStack(),\n",
    "                                     ToFloat32(),\n",
    "                                     AdaptiveResize((64, 64)),\n",
    "                                     Unstack(),\n",
    "                                     ])\n",
    "hmdb51_data_0 = torchvision.datasets.HMDB51(root=\"hmdb51/data_0\",\n",
    "                                          annotation_path=\"hmdb51/annotation_0\",\n",
    "                                          frames_per_clip=frames_per_clip,\n",
    "                                          step_between_clips=step_between_clips,\n",
    "                                          transform=data_transform\n",
    "                                          )\n",
    "data_loader_0 = torch.utils.data.DataLoader(hmdb51_data_0,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=shuffle,\n",
    "                                          )\n",
    "hmdb51_data_1 = torchvision.datasets.HMDB51(root=\"hmdb51/data_1\",\n",
    "                                          annotation_path=\"hmdb51/annotation_1\",\n",
    "                                          frames_per_clip=frames_per_clip,\n",
    "                                          step_between_clips=step_between_clips,\n",
    "                                          transform=data_transform\n",
    "                                          )\n",
    "data_loader_1 = torch.utils.data.DataLoader(hmdb51_data_1,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=shuffle,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_0, _, _ = next(iter(data_loader_0))\n",
    "label_0 = torch.zeros(len(video_0))\n",
    "plt.imshow(video_0[0][:,0,:,:].permute(1, 2, 0))\n",
    "print(f\"Video batch shape (N, C, D, H, W) :{video_0.shape}, Labels:{label_0}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_1, _, _ = next(iter(data_loader_1))\n",
    "label_1 = torch.ones(len(video_1))\n",
    "plt.imshow(video_1[0][:,0,:,:].permute(1, 2, 0))\n",
    "print(f\"Video batch shape (N, C, D, H, W) :{video_1.shape}, Labels:{label_1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for step_i in range(int(400e3)):\n",
    "    video_0, _, _ = next(iter(data_loader_0))\n",
    "    label_0 = torch.zeros((len(video_0), 1))\n",
    "    video_1, _, _ = next(iter(data_loader_1))\n",
    "    label_1 = torch.ones((len(video_1), 1))\n",
    "    model.train_step(video_0.to(device), label_0.to(device), video_1.to(device), label_1.to(device))\n",
    "\n",
    "model.save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}